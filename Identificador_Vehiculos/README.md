# üöó Proyecto: Identificador de Veh√≠culos con Transfer Learning

##Resumen del Proyecto
[cite_start]Este proyecto desarrolla un sistema de **Visi√≥n por Computadora** para la clasificaci√≥n de veh√≠culos en im√°genes[cite: 115]. [cite_start]Se utiliz√≥ la arquitectura **Xception** a trav√©s de Transfer Learning [cite: 133, 134] [cite_start]y un pipeline implementado en Python, ejecutado desde R mediante la librer√≠a `reticulate`[cite: 116]. [cite_start]El objetivo fue construir un detector de veh√≠culos funcional, ajustando librer√≠as (usando Keras3) [cite: 153] [cite_start]y aplicando t√©cnicas avanzadas de optimizaci√≥n para el entrenamiento[cite: 141, 145].

##Stack Tecnol√≥gico

| Componente | Herramientas/Librer√≠as | Descripci√≥n |
| :--- | :--- | :--- |
| **Modelo Base** | Xception | [cite_start]Utilizado mediante Transfer Learning, con pesos congelados y entrenado con ImageNet[cite: 133, 134]. |
| **Frameworks** | Keras3, TensorFlow | [cite_start]Librer√≠as principales para la construcci√≥n y entrenamiento del modelo, elegidas para resolver problemas de compatibilidad[cite: 153]. |
| **Lenguajes** | R, Python | [cite_start]El pipeline de entrenamiento y deployment fue ejecutado desde R usando Python a trav√©s de `reticulate`[cite: 116, 121]. |
| **Deployment** | Shiny | [cite_start]Utilizado para montar la interfaz gr√°fica del clasificador (`app.R`)[cite: 116]. |

##Procesamiento y Entrenamiento
[cite_start]El conjunto de datos utilizado fue ‚ÄúVehicle Image Classification‚Äù (Kaggle), complementado con im√°genes de camiones para ampliar el reconocimiento[cite: 124, 125].

### 1. Preprocesamiento y Aumento de Datos
* [cite_start]**Aumento de Datos (*Data Augmentation*):** Se aplicaron volteos horizontales aleatorios y rotaciones leves (hasta el 10% del rango total)[cite: 127]. [cite_start]Esto expandi√≥ artificialmente el dataset, mejorando la robustez[cite: 128].
* [cite_start]**Normalizaci√≥n:** Los valores de p√≠xeles se reescalaron de $[0, 255]$ a $[0.0, 1.0]$ dividiendo por 255[cite: 129, 130].
* [cite_start]**Prefetching:** Se implement√≥ para optimizar la entrada de datos, cargando lotes de im√°genes de forma as√≠ncrona[cite: 131].

### 2. Optimizaci√≥n y Control de Entrenamiento (Callbacks)
[cite_start]Para maximizar la eficiencia en 4 √©pocas [cite: 150] y evitar el sobreajuste (*overfitting*), se definieron dos mecanismos de control:

* [cite_start]**Early Stopping:** Monitoreando la P√©rdida de Validaci√≥n (`val_loss`)[cite: 142]. [cite_start]Se estableci√≥ una `patience = 3` y se us√≥ `restore_best_weights = TRUE` para asegurar que el modelo final fuese la mejor versi√≥n encontrada (la de menor `val_loss`)[cite: 143, 144].
* [cite_start]**Reduce Learning Rate On Plateau:** Si el rendimiento no mejora durante dos √©pocas (`patience = 2`), la **Tasa de Aprendizaje** se reduce autom√°ticamente al 50%[cite: 147, 148]. [cite_start]Esto permite que el optimizador encuentre el m√≠nimo error con mayor estabilidad y precisi√≥n[cite: 149].

### üìä Resultados del Modelo
* [cite_start]**M√©trica de Error:** Entrop√≠a Cruzada Categ√≥rica (`Categorical Crossentropy`)[cite: 136].
* [cite_start]**Optimizador:** Adam[cite: 138].
* [cite_start]**M√©trica de Rendimiento:** Exactitud (`Accuracy`)[cite: 140].

El entrenamiento se detuvo en la √âpoca 4 por las condiciones de *Early Stopping*.

| M√©trica | Conjunto de Entrenamiento (√âpoca 4) | Conjunto de Validaci√≥n (√âpoca 3 - Mejor Peso) |
| :--- | :--- | :--- |
| **Exactitud (Accuracy)** | 99% (M√°ximo en la √âpoca 4) | [cite_start]~98.5% (M√°ximo en la √âpoca 3) [cite: 150] |
| **P√©rdida (Cross-Entropy)** | ~0.04 (M√≠nimo en la √âpoca 4) | [cite_start]~0.06 (M√≠nimo en la √âpoca 3) [cite: 150] |

##Aprendizajes Clave
[cite_start]Este proyecto fue fundamental para comprender y aplicar estrategias avanzadas de entrenamiento y manejo de dependencias[cite: 155].
* [cite_start]**Gesti√≥n de Dependencias y Compatibilidad:** Se resolvi√≥ la incompatibilidad entre la versi√≥n de R y las librer√≠as Keras/TensorFlow al investigar y optar por **Keras3**[cite: 153].
* [cite_start]**Optimizaci√≥n del Entrenamiento:** El uso de **Early Stopping** y **Reduce Learning Rate On Plateau** permiti√≥ ajustar de manera √≥ptima y robusta la cantidad de √©pocas, ahorrando tiempo y evitando el sobreajuste (*overfitting*)[cite: 156].

---

**[Enlace a la carpeta del script de entrenamiento]**
**[Enlace a la carpeta de la aplicaci√≥n Shiny (Deployment)]**
